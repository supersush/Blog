---
layout: post
title: "重温SVM"
author: "sush"
header-img: "img/post-bg-web.jpg"
header-mask: 0.4
tags:
  - SVM
---
### **SVM学习笔记**
#### **前言** ####
SVM是一个古老的算法，特别擅长于特征维度多余样本维度的情况。虽然随着深度学习的崛起，SVM有没落的趋势，但是SVM是一个理论非常完备的算法，系统的学习SVM对于我们掌握凸优化，约束条件，松弛条件，用计算机求解线性规划等问题有很大的帮助。

#### **最优分离超平面定义** ####
<img src="http://latex.codecogs.com/gif.latex? \min_{\beta} \frac{1}{2}||\beta||^2">
<img src="http://latex.codecogs.com/gif.latex? s.t. -y_i(\beta^T x_i + \beta_0)<=-1,i=1,2,...,N">

#### **原问题的拉格朗日对偶问题** ####
拉格朗日函数为：
<img src="http://latex.codecogs.com/gif.latex? L(\beta,\beta_0;\alpha)=\frac{1}{2}\beta^T\beta+\sum_{i=1}^{N}\alpha_i(-y_i(\beta^T x_i+\beta_0)+1)">
分别对beta和beta_0求导：  
<img src="http://latex.codecogs.com/gif.latex? \nabla_\beta L(\beta,\beta_0;a)=\beta-\sum_{i=1}^{N}a_i y_i x_i=0">
<img src="http://latex.codecogs.com/gif.latex? \frac{\partial L}{\partial \beta_0}=-\sum_{i=1}^{N}a_i y_i=0">
根据KKT条件有：
<img src="http://latex.codecogs.com/gif.latex? \alpha_i>=0,\forall i">
<img src="http://latex.codecogs.com/gif.latex? \alpha_i(-y_i(\beta^T x_i+\beta_0)+1)=0,\forall i">
根据以上四个条件化简拉格朗日函数，得到拉格朗日对偶函数：  
<img src="http://latex.codecogs.com/gif.latex? g(a)=\inf_{\beta,\beta_0}L(\beta,\beta_0;\alpha)=\sum_{i=1}^N \alpha_i-\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j">
所以原问题的对偶问题为：
<img src="http://latex.codecogs.com/gif.latex? \max_\alpha g(\alpha)">
<img src="http://latex.codecogs.com/gif.latex? s.t.\ \alpha_i>=0,i=1,2,...,N">
<img src="http://latex.codecogs.com/gif.latex? \sum_{i=1}^{N}\alpha_i y_i=0">
<img src="http://latex.codecogs.com/gif.latex? \alpha_i(-y_i(\beta^T x_i+\beta_0)+1)=0,\forall i">
之所以引入对偶问题是因为对偶问题更容易求解，原问题的求解与特征的维度和样本数有关，对偶问题只与样本数有关

#### **内点法求解拉格朗日对偶问题** ####
定义函数：
<img src="http://latex.codecogs.com/gif.latex? h(\alpha,\lambda;t)=-\sum_{i=1}^{N}\alpha_i+\frac{1}{2}\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j x_i^T X_j + \sum_{i=1}^N -\frac{1}{t} log\alpha_i+\lambda\sum_{i=1}^{N}\alpha_i y_i">




[一篇很赞的文章，讲解了SVM以及和其他模型的比较](https://zhuanlan.zhihu.com/p/93715996)  






